export const metadata = {
  title: 'My Dev Tooling in 2025: A Year of Agents',
  description: 'A quarter by quarter breakdown of how my development workflow evolved in 2025, from DeepSeek R1 to Claude Code.',
  tags: ["tech", "dev tooling"],
  hero: false,
  date: "10/10/2025",
};

# My Dev Tooling in 2025: A Year of Agents

As of October 10, 2025, my development stack feels nothing like the one I was running in January. Terminal native agents, reasoning models, and Rust backed tooling moved from weekend experiments to the set of tools I trust every day. This write up is me keeping score on what actually changed, quarter by quarter, and why I stayed with each shift.

***

## Q1 2025 (Jan to Mar): Reasoning Models Meet Reality

January 20 was the day DeepSeek R1 arrived. It was billed as the open reasoning model that could stand next to OpenAI o1, so of course I pointed it at real work through Fireworks AI the moment I could. The promise was obvious. Finally, a reasoning model I could run on my own terms, wrap with my own scaffolding, and tune without a waiting list. The reality still demanded guardrails. Long rollouts lost the plot unless I guided every decision, and letting the agent call tools without close supervision went sideways more often than it landed. Still, that release made it clear that scaling test time compute and reinforcement learning is pulling open models into the same arena as the big proprietary systems.

While I was wrestling with that, Zed stayed bolted to my dock. Its Rust core and GPU aware rendering meant no mysterious pauses, and the collaboration features kept remote pairing with teammates and agents simple. Q1 ended with agents mostly living in chat sidecars while I kept typing, but the rumble under the surface was impossible to ignore.

***

## Q2 2025 (Apr to Jun): Python Tooling Rebuilt in Rust

Early spring is when Astral shipped enough updates to `uv` that it went from a curiosity to the primary way I touch Python projects. One fast Rust binary now handles Python installs, virtual environments, and script execution, so the old stack of `pyenv`, `pip`, and custom shell scripts finally left my muscle memory. I kept seeing `uv` pop up in new open source repos, and it felt like the ecosystem agreed.

The killer feature remains inline script dependencies. Instead of stopping to spin up another virtual environment, I drop a short header at the top of whatever quick script I am hacking on and let `uv run` do the rest.

```python
# /// script
# dependencies = [
#   "requests",
#   "beautifulsoup4",
# ]
# ///

import requests
from bs4 import BeautifulSoup
```

Before `uv`, that same throwaway script meant building and activating an environment, installing packages, then remembering to tear it down later. Now the header plus a single command handles the entire lifecycle. Python slid back in as my default language for fast automation because the setup overhead finally vanished.

***

## Q3 2025 (Jul to Sep): Agents Stop Feeling Experimental

July 17 is when the term agent stopped being marketing fluff. OpenAI launched the ChatGPT agent and suddenly there were ready made workflows for research, repo triage, and structured planning. Less than three weeks later the GPT 5 update arrived with sharper coding instincts and steadier multimodal reasoning, and Cursor dropped its Agent CLI so I could let background helpers refactor or document while I stayed focused on higher level design. Anthropic answered with Claude Opus 4.1 and a Chrome extension that can steer the browser, then wrapped the quarter on September 29 with Claude Sonnet 4.5, which is still the most reliable coding model in my rotation.

Claude Code is the tool that hooked me. The sessions stay stable thanks to automatic memory compacting, it rarely forgets where it is in the plan, and even without formal checkpoints I can ask it to undo a step and it gets back on track. The unlimited request plan removed any hesitation about iterating until something felt right. I caught myself reaching for Claude Code instead of my IDE for everyday work, and that was a first.

It works best when it can pull its own context, so I loosened the config to give it more room to think:

```json
{
  "env": {
    "ANTHROPIC_MODEL": "claude_sonnet_4_5_20250929",
    "CLAUDE_CODE_MAX_OUTPUT_TOKENS": 60000,
    "DISABLE_TELEMETRY": 1,
    "CLAUDE_CODE_ENABLE_TELEMETRY": 0,
    "MAX_THINKING_TOKENS": "40000",
    "BASH_DEFAULT_TIMEOUT_MS": "900000",
    "BASH_MAX_TIMEOUT_MS": "900000"
  },
  "permissions": {
    "defaultMode": "plan",
    "allow": [
      "WebFetch",
      "WebSearch",
      "Bash(rg:*)",
      "Bash(bunx:*)",
      "Bash(find:*)",
      "Bash(grep:*)",
      "Bash(date:*)",
      "Bash(echo:*)",
      "Bash(tail:*)",
      "Bash(head:*)",
      "Bash(cat:*)",
      "Bash(mkdir:*)",
      "Bash(curl:*)",
      "Bash(gh search:*)",
      "Bash(gh pr view:*)",
      "Bash(gh repo view:*)",
      "Bash(pdftotext:*)",
      "Bash(cargo check:*)",
      "Bash(npm view:*)",
      "Bash(wc:*)",
      "Bash(sed:*)",
      "Bash(astgrep:*)",
      "Bash(ls:*)"
    ]
  },
  "alwaysThinkingEnabled": true,
  "feedbackSurveyState": {
    "lastShownTime": 1754023673876
  }
}
```

Even with that, recency still wins. I lean on the `/init` command to help the model build a mental map of the repository, yet a big architectural change will still produce stale guesses until it rereads the relevant files. Agents have incredible momentum, but their memory is still fragile.

***

## Q4 2025 (Oct to Dec): Watching the Next Wave Form

We are only ten days into Q4 as I write this and the release cadence is still accelerating. OpenAI used DevDay to ship AgentKit, refresh Codex fine tuning, and position agents at the center of every roadmap slide. Anthropic is already previewing whatever follows Sonnet 4.5, and the usual Twelve Days of Shipmas rumor mill says OpenAI is not done. This quarter feels like the setup for everything we will be asked to wrangle in 2026.

***

## Coming Next

I am putting together a follow up that zooms in on the weeks I let agents hold the keyboard. It will cover the moments where that felt like a productivity superpower, the lapses that burned time instead, and the guardrails I am building before we do this all over again next year.
